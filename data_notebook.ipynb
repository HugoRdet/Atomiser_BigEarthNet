{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e28fc5-b768-412c-8e0a-91aece41507a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12a2b3530>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training.perceiver import*\n",
    "from training.utils import*\n",
    "from training.losses import*\n",
    "from training.VIT import*\n",
    "from training.ResNet import*\n",
    "from collections import defaultdict\n",
    "from training import*\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import einops as einops\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Reduce\n",
    "from pytorch_lightning.profilers import AdvancedProfiler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from configilm import util\n",
    "util.MESSAGE_LEVEL = util.MessageLevel.INFO  # use INFO to see all messages\n",
    "\n",
    "\n",
    "from configilm.extra.DataSets import BENv2_DataSet\n",
    "from configilm.extra.DataModules import BENv2_DataModule\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7188037a-09cf-4063-a849-c4f9c5ddb2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=\"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cc97c1-2090-4da3-a929-18ff5737e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_BENv2(mode=None,max_len=None):\n",
    "    dico_paths={\"images_lmdb\":\"data/Encoded-BigEarthNet/\",\n",
    "    \"metadata_parquet\":\"data/Encoded-BigEarthNet/metadata.parquet\",\n",
    "    \"metadata_snow_cloud_parquet\":\"data/Encoded-BigEarthNet/metadata_for_patches_with_snow_cloud_or_shadow.parquet\"}\n",
    "\n",
    "    df=open_parquet(dico_paths[\"metadata_parquet\"])\n",
    "\n",
    "    if max_len!=None:\n",
    "        return BENv2_DataSet.BENv2DataSet(data_dirs=dico_paths, img_size=(14, 120, 120),split=mode,max_len=max_len),df\n",
    "\n",
    "    \n",
    "    return BENv2_DataSet.BENv2DataSet(data_dirs=dico_paths, img_size=(14, 120, 120),split=mode),df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc046e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(name,trans_conf,sizes=(2,2,2),max_len=None,max_len_h5=-1):\n",
    "\n",
    "   \n",
    "    mode=\"train\"\n",
    "    ds,df=prepare_BENv2(max_len=max_len)\n",
    "\n",
    "    idxs,_=get_tiny_dataset(ds,df,MAX_IDs=sizes[0],mode=mode)\n",
    "    #idxs=None\n",
    "    stats=create_dataset(idxs, ds,df, name=name, mode=mode, trans_config=trans_conf,stats=None,max_len=max_len_h5)\n",
    "\n",
    "    mode=\"validation\"\n",
    "    ds,df=prepare_BENv2(max_len=max_len)\n",
    "    \n",
    "    \n",
    "    idxs,_=get_tiny_dataset(ds,df,MAX_IDs=sizes[1],mode=mode)\n",
    "    #idxs=None\n",
    "    create_dataset(idxs, ds,df, name=name, mode=mode,trans_config=trans_conf,stats=stats,max_len=max_len_h5)\n",
    "\n",
    "    mode=\"test\"\n",
    "    ds,_=prepare_BENv2(max_len=max_len)\n",
    "    idxs,_=get_tiny_dataset(ds,df,MAX_IDs=sizes[2],mode=mode)\n",
    "    #idxs=None\n",
    "    create_dataset(idxs, ds,df, name=name, mode=mode,trans_config=trans_conf,stats=None,max_len=max_len_h5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ebd737",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_yaml=\"./data/bands_info/bands.yaml\"\n",
    "configs_dataset=\"./data/Tiny_BigEarthNet/configs_dataset_regular.yaml\"\n",
    "config_dico = read_yaml(\"./training/configs/config_test-Atomiser_Atos.yaml\")\n",
    "#test_conf= transformations_config(config_dico,bands_yaml,configs_dataset,path_imgs_config=\"./data/Tiny_BigEarthNet/\",name_config=\"BigEarthPart\")\n",
    "#(self,configs_dataset,path_imgs_config,name_config=\"\"):\n",
    "modalities_trans= modalities_transformations_config(configs_dataset,name_config=\"regular\")\n",
    "test_conf= transformations_config(bands_yaml,config_dico)\n",
    "\n",
    "#create_datasets(\"regular\",modalities_trans,sizes=(5,5,5),max_len_h5=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152e05cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader created on rank: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_module=Tiny_BigEarthNetDataModule( \"./data/Tiny_BigEarthNet/regular\", batch_size=16, num_workers=4,trans_modalities=modalities_trans,model=\"Atomiser\")\n",
    "\n",
    "data_module.setup()\n",
    "# Prepare dataloaders\n",
    "train_loader = data_module.train_dataloader()\n",
    "#val_loader = data_module.val_dataloader()\n",
    "#test_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c8d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "xp_name=\"test_xp\"\n",
    "config_model = \"Atomiser_Atos\"\n",
    "config_name_dataset = \"tiny\"\n",
    "config_name_dataset= \"./data/custom_flair/\"+config_name_dataset\n",
    "from pytorch_lightning import Trainer,seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "config_model = read_yaml(\"./training/configs/config_test-\"+config_model+\".yaml\")\n",
    "#labels=load_json_to_dict(\"./data/flair_2_toy_dataset/flair_labels.json\")\n",
    "bands_yaml = \"./data/Tiny_BigEarthNet/bands.yaml\"\n",
    "\n",
    "bands_yaml=\"./data/bands_info/bands.yaml\"\n",
    "configs_dataset=\"./data/Tiny_BigEarthNet/configs_dataset_regular.yaml\"\n",
    "config_dico = read_yaml(\"./training/configs/config_test-Atomiser_Atos.yaml\")\n",
    "\n",
    "modalities_trans= modalities_transformations_config(configs_dataset,name_config=\"regular\")\n",
    "test_conf= transformations_config(bands_yaml,config_dico)\n",
    "       \n",
    "data_module=Tiny_BigEarthNetDataModule( \"./data/Tiny_BigEarthNet/regular\", batch_size=16, num_workers=4,trans_modalities=modalities_trans,model=\"Atomiser\")\n",
    "\n",
    "data_module.setup()\n",
    "# Prepare dataloaders\n",
    "\n",
    "wand = False\n",
    "wandb_logger = None\n",
    "if wand:\n",
    "    if os.environ.get(\"LOCAL_RANK\", \"0\") == \"0\":\n",
    "        import wandb\n",
    "        wandb.init(\n",
    "            name=config_model['encoder'],\n",
    "            project=config_name_dataset+\"_modalities\",\n",
    "            config=config_model\n",
    "        )\n",
    "        wandb_logger = WandbLogger(project=config_name_dataset+\"_modalities\")\n",
    "\n",
    "#def __init__(self, config, wand, name)\n",
    "model = Model(config_model,wand=wand, name=xp_name,transform=test_conf)\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_ap\", min_delta=0.00, patience=15, verbose=False, mode=\"max\")\n",
    "\n",
    "profiler = AdvancedProfiler(dirpath=\"profiling\", filename=\"profiler_output.txt\")\n",
    "\n",
    "# Configure the trainer for distributed training.\n",
    "trainer = Trainer(\n",
    "    use_distributed_sampler=False,  # we use our custom sampler\n",
    "    #strategy=\"ddp\",\n",
    "    max_epochs=config_model[\"trainer\"][\"epochs\"],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=[early_stop_callback],\n",
    "    profiler=profiler  \n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363674a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/hugoriffaud/Documents/Atomiser_BigEarthNet/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name                            | Type                       | Params | Mode \n",
      "---------------------------------------------------------------------------------------\n",
      "0 | transform                       | transformations_config     | 0      | train\n",
      "1 | metric_train_AP_per_class       | MultilabelAveragePrecision | 0      | train\n",
      "2 | metric_train_accuracy_per_class | MultilabelAccuracy         | 0      | train\n",
      "3 | metric_val_AP_per_class         | MultilabelAveragePrecision | 0      | train\n",
      "4 | metric_val_accuracy_per_class   | MultilabelAccuracy         | 0      | train\n",
      "5 | metric_test_AP_per_class        | MultilabelAveragePrecision | 0      | train\n",
      "6 | metric_test_accuracy_per_class  | MultilabelAccuracy         | 0      | train\n",
      "7 | encoder                         | Atomiser                   | 15.6 M | train\n",
      "8 | loss                            | BCEWithLogitsLoss          | 0      | train\n",
      "---------------------------------------------------------------------------------------\n",
      "15.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 M    Total params\n",
      "62.583    Total estimated model params size (MB)\n",
      "106       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea99479f7e64c0384d4740da72eb392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader created on rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/Atomiser_BigEarthNet/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0016 seconds\n",
      "[Timing] process_data took 0.0015 seconds\n",
      "[Timing] process_data took 0.0018 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0015 seconds\n",
      "[Timing] process_data took 0.0014 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0014 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0008 seconds\n",
      "[Timing] process_data took 0.0008 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0016 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0014 seconds\n",
      "[Timing] process_data took 0.0015 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0008 seconds\n",
      "[Timing] process_data took 0.0014 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0018 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0014 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0008 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0016 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "Train DataLoader created on rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/Atomiser_BigEarthNet/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bed9afe9b0a4eb1b4fad901211f6384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0009 seconds\n",
      "[Timing] process_data took 0.0017 seconds\n",
      "[Timing] process_data took 0.0015 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0014 seconds\n",
      "[Timing] process_data took 0.0014 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0007 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0018 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0017 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0018 seconds\n",
      "[Timing] process_data took 0.0016 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0008 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0013 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0006 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0004 seconds\n",
      "[Timing] process_data took 0.0004 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0011 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0012 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0010 seconds\n",
      "[Timing] process_data took 0.0005 seconds\n",
      "[Timing] process_data took 0.0004 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec496c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
