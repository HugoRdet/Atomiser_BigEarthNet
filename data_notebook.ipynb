{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e28fc5-b768-412c-8e0a-91aece41507a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11a2cf550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training.perceiver import*\n",
    "from training.utils import*\n",
    "from training.losses import*\n",
    "from training.VIT import*\n",
    "from training.ResNet import*\n",
    "from collections import defaultdict\n",
    "from training import*\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import einops as einops\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Reduce\n",
    "from pytorch_lightning.profilers import AdvancedProfiler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from configilm import util\n",
    "util.MESSAGE_LEVEL = util.MessageLevel.INFO  # use INFO to see all messages\n",
    "\n",
    "\n",
    "from configilm.extra.DataSets import BENv2_DataSet\n",
    "from configilm.extra.DataModules import BENv2_DataModule\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7188037a-09cf-4063-a849-c4f9c5ddb2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=\"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cc97c1-2090-4da3-a929-18ff5737e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_BENv2(mode=None,max_len=None):\n",
    "    dico_paths={\"images_lmdb\":\"data/Encoded-BigEarthNet/\",\n",
    "    \"metadata_parquet\":\"data/Encoded-BigEarthNet/metadata.parquet\",\n",
    "    \"metadata_snow_cloud_parquet\":\"data/Encoded-BigEarthNet/metadata_for_patches_with_snow_cloud_or_shadow.parquet\"}\n",
    "\n",
    "    df=open_parquet(dico_paths[\"metadata_parquet\"])\n",
    "\n",
    "    if max_len!=None:\n",
    "        return BENv2_DataSet.BENv2DataSet(data_dirs=dico_paths, img_size=(14, 120, 120),split=mode,max_len=max_len),df\n",
    "\n",
    "    \n",
    "    return BENv2_DataSet.BENv2DataSet(data_dirs=dico_paths, img_size=(14, 120, 120),split=mode),df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc046e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(name,trans_conf,trans_tokens,sizes=(2,2,2),max_len=None,max_len_h5=-1):\n",
    "\n",
    "   \n",
    "    mode=\"train\"\n",
    "    ds,df=prepare_BENv2(max_len=max_len)\n",
    "\n",
    "    idxs,_=get_tiny_dataset(ds,df,MAX_IDs=sizes[0],mode=mode)\n",
    "    #idxs=None\n",
    "    stats=create_dataset(idxs, ds,df, name=name, mode=mode, trans_config=trans_conf,trans_tokens=trans_tokens,stats=None,max_len=max_len_h5)\n",
    "\n",
    "    mode=\"validation\"\n",
    "    ds,df=prepare_BENv2(max_len=max_len)\n",
    "    \n",
    "    \n",
    "    idxs,_=get_tiny_dataset(ds,df,MAX_IDs=sizes[1],mode=mode)\n",
    "    #idxs=None\n",
    "    create_dataset(idxs, ds,df, name=name, mode=mode,trans_config=trans_conf,trans_tokens=trans_tokens,stats=stats,max_len=max_len_h5)\n",
    "\n",
    "    mode=\"test\"\n",
    "    ds,_=prepare_BENv2(max_len=max_len)\n",
    "    idxs,_=get_tiny_dataset(ds,df,MAX_IDs=sizes[2],mode=mode)\n",
    "    #idxs=None\n",
    "    create_dataset(idxs, ds,df, name=name, mode=mode,trans_config=trans_conf,trans_tokens=trans_tokens,stats=stats,max_len=max_len_h5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ebd737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m[INFO]    Loading BEN data for None...\u001b[0m\n",
      "\u001b[96m[INFO]        480038 patches indexed\u001b[0m\n",
      "\u001b[96m[INFO]        480038 pre-filtered patches indexed\u001b[0m\n",
      "\u001b[96m[INFO]        480038 filtered patches indexed\u001b[0m\n",
      "\u001b[96m[INFO]    Merged metadata with snow/cloud metadata\u001b[0m\n",
      "\u001b[96m[INFO]    Loaded 549488 labels\u001b[0m\n",
      "\u001b[96m[INFO]    Loaded 549488 keys\u001b[0m\n",
      "\u001b[96m[INFO]    Loaded mapping created\u001b[0m\n",
      "\u001b[96m[INFO]    Opening LMDB environment ...\u001b[0m\n",
      "summary dico stats\n",
      "{18: 1000, 12: 1000, 8: 1000, 6: 1000, 1: 1000, 11: 1000, 15: 1000, 9: 1000, 14: 1000, 13: 1000, 3: 1000, 16: 1000, 17: 1000, 0: 1000, 5: 1000, 4: 679, 10: 1000, 7: 1000, 2: 694}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/Tiny_BigEarthNet/transformations/regular/train/410_transfos_train.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m modalities_trans= modalities_transformations_config(configs_dataset,name_config=\u001b[33m\"\u001b[39m\u001b[33mregular\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m test_conf= transformations_config(bands_yaml,config_dico)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mcreate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mregular\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodalities_trans\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrans_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43msizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_len_h5\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcreate_datasets\u001b[39m\u001b[34m(name, trans_conf, trans_tokens, sizes, max_len, max_len_h5)\u001b[39m\n\u001b[32m      7\u001b[39m idxs,_=get_tiny_dataset(ds,df,MAX_IDs=sizes[\u001b[32m0\u001b[39m],mode=mode)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#idxs=None\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m stats=\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrans_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrans_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrans_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_len_h5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m mode=\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m ds,df=prepare_BENv2(max_len=max_len)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Atomiser_BigEarthNet/training/utils/utils_dataset_h5.py:157\u001b[39m, in \u001b[36mcreate_dataset\u001b[39m\u001b[34m(dico_idxs, ds, df, name, mode, trans_config, trans_tokens, stats, max_len)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_dataset\u001b[39m(dico_idxs, ds,df, name=\u001b[33m\"\u001b[39m\u001b[33mtiny\u001b[39m\u001b[33m\"\u001b[39m, mode=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,trans_config=\u001b[38;5;28;01mNone\u001b[39;00m,trans_tokens=\u001b[38;5;28;01mNone\u001b[39;00m, stats=\u001b[38;5;28;01mNone\u001b[39;00m,max_len=-\u001b[32m1\u001b[39m):\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dico_idxs!=\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_dataset_dico\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdico_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrans_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrans_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrans_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# 1) Clean up any existing file\u001b[39;00m\n\u001b[32m    162\u001b[39m     h5_path = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./data/Tiny_BigEarthNet/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.h5\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Atomiser_BigEarthNet/training/utils/utils_dataset_h5.py:310\u001b[39m, in \u001b[36mcreate_dataset_dico\u001b[39m\u001b[34m(dico_idxs, ds, name, mode, trans_config, trans_tokens, stats)\u001b[39m\n\u001b[32m    307\u001b[39m img, label = ds[elem_id]  \u001b[38;5;66;03m# shape (12, 120, 120)\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trans_config!=\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[43mtrans_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_transform_image_dico\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodality_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m     trans_config.create_transform_image_dico(\u001b[38;5;28mint\u001b[39m(elem_id),mode=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m,modality_folder=mode)\n\u001b[32m    312\u001b[39m     trans_config.create_transform_image_dico(\u001b[38;5;28mint\u001b[39m(elem_id),mode=\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m,modality_folder=mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Atomiser_BigEarthNet/training/utils/utils_modalities_transformation.py:182\u001b[39m, in \u001b[36mmodalities_transformations_config.create_transform_image_dico\u001b[39m\u001b[34m(self, img_idx, mode, modality_folder)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name_config!=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    181\u001b[39m     save_path=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodality_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_transfos_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.yaml\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[43msave_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_dico\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Atomiser_BigEarthNet/training/utils/files_utils.py:28\u001b[39m, in \u001b[36msave_yaml\u001b[39m\u001b[34m(path, data)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_yaml\u001b[39m(path, data):\n\u001b[32m     22\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    Save a dictionary as a YAML file at the specified path.\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    :param path: The file path where the YAML file will be saved.\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    :param data: The dictionary to save as YAML.\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     29\u001b[39m         yaml.safe_dump(data, file, sort_keys=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './data/Tiny_BigEarthNet/transformations/regular/train/410_transfos_train.yaml'"
     ]
    }
   ],
   "source": [
    "bands_yaml=\"./data/bands_info/bands.yaml\"\n",
    "configs_dataset=\"./data/Tiny_BigEarthNet/configs_dataset_regular.yaml\"\n",
    "config_dico = read_yaml(\"./training/configs/config_test-Atomiser_Atos.yaml\")\n",
    "#test_conf= transformations_config(config_dico,bands_yaml,configs_dataset,path_imgs_config=\"./data/Tiny_BigEarthNet/\",name_config=\"BigEarthPart\")\n",
    "#(self,configs_dataset,path_imgs_config,name_config=\"\"):\n",
    "modalities_trans= modalities_transformations_config(configs_dataset,name_config=\"regular\")\n",
    "test_conf= transformations_config(bands_yaml,config_dico)\n",
    "\n",
    "create_datasets(\"regular\",modalities_trans,trans_tokens=test_conf,sizes=(1000,1000,1000),max_len_h5=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152e05cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampler initialization: 100%|██████████| 95/95 [00:00<00:00, 933158.97it/s]\n",
      "Batch creation: 100%|██████████| 3/3 [00:00<00:00, 33644.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader created on rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_module=Tiny_BigEarthNetDataModule( \"./data/Tiny_BigEarthNet/regular\", \n",
    "                                       batch_size=16, \n",
    "                                       num_workers=4,\n",
    "                                       trans_modalities=modalities_trans,\n",
    "                                       trans_tokens=test_conf,\n",
    "                                       model=\"Atomiser\")\n",
    "\n",
    "data_module.setup()\n",
    "# Prepare dataloaders\n",
    "train_loader = data_module.train_dataloader()\n",
    "#val_loader = data_module.val_dataloader()\n",
    "#test_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4c8d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/Users/hugoriffaud/Documents/Atomiser_BigEarthNet/venv/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:209: Attribute 'transform' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transform'])`.\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/hugoriffaud/Documents/Atomiser_BigEarthNet/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name                            | Type                       | Params | Mode \n",
      "---------------------------------------------------------------------------------------\n",
      "0 | transform                       | transformations_config     | 0      | train\n",
      "1 | metric_train_AP_per_class       | MultilabelAveragePrecision | 0      | train\n",
      "2 | metric_train_accuracy_per_class | MultilabelAccuracy         | 0      | train\n",
      "3 | metric_val_AP_per_class         | MultilabelAveragePrecision | 0      | train\n",
      "4 | metric_val_accuracy_per_class   | MultilabelAccuracy         | 0      | train\n",
      "5 | metric_test_AP_per_class        | MultilabelAveragePrecision | 0      | train\n",
      "6 | metric_test_accuracy_per_class  | MultilabelAccuracy         | 0      | train\n",
      "7 | encoder                         | Atomiser                   | 2.1 M  | train\n",
      "8 | loss                            | BCEWithLogitsLoss          | 0      | train\n",
      "---------------------------------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.331     Total estimated model params size (MB)\n",
      "268       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9522f7677baf427995e5c3d5022e5e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampler initialization: 100%|██████████| 95/95 [00:00<00:00, 641640.71it/s]\n",
      "Batch creation: 100%|██████████| 3/3 [00:00<00:00, 29468.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader created on rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/hugoriffaud/Documents/Atomiser_BigEarthNet/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 72000, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampler initialization: 100%|██████████| 95/95 [00:00<00:00, 804967.43it/s]\n",
      "Batch creation: 100%|██████████| 3/3 [00:00<00:00, 29676.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader created on rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/hugoriffaud/Documents/Atomiser_BigEarthNet/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bada9da49b4dec898a0ef15fccd664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 14580, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 43200, 30])\n",
      "torch.Size([16, 14580, 30])\n",
      "torch.Size([16, 139968, 30])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da30522281484081a16f44dd03c19121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 27648, 30])\n",
      "torch.Size([16, 43200, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 14580, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 14580, 30])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e533f67e53d242cf9fbb7c03f8496b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 27648, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 43200, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 14580, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 14580, 30])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969ba79921d24af1864424328b3c12a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 27648, 30])\n",
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 14580, 30])\n",
      "torch.Size([16, 43200, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 14580, 30])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964c294643ab4c83a7fd8f4773700f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 27648, 30])\n",
      "torch.Size([16, 14580, 30])\n",
      "torch.Size([16, 43200, 30])\n",
      "torch.Size([16, 14580, 30])\n",
      "torch.Size([16, 139968, 30])\n",
      "torch.Size([16, 139968, 30])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f73dfb6cb364d29874024347b4aa8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 72000, 30])\n",
      "torch.Size([16, 139968, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 27648, 30])\n"
     ]
    }
   ],
   "source": [
    "xp_name=\"test_xp\"\n",
    "config_model = \"Atomiser_Atos\"\n",
    "config_name_dataset = \"tiny\"\n",
    "config_name_dataset= \"./data/custom_flair/\"+config_name_dataset\n",
    "from pytorch_lightning import Trainer,seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "config_model = read_yaml(\"./training/configs/config_test-\"+config_model+\".yaml\")\n",
    "#labels=load_json_to_dict(\"./data/flair_2_toy_dataset/flair_labels.json\")\n",
    "bands_yaml = \"./data/Tiny_BigEarthNet/bands.yaml\"\n",
    "\n",
    "bands_yaml=\"./data/bands_info/bands.yaml\"\n",
    "configs_dataset=\"./data/Tiny_BigEarthNet/configs_dataset_regular.yaml\"\n",
    "config_dico = read_yaml(\"./training/configs/config_test-Atomiser_Atos.yaml\")\n",
    "\n",
    "modalities_trans= modalities_transformations_config(configs_dataset,name_config=\"regular\")\n",
    "test_conf= transformations_config(bands_yaml,config_dico)\n",
    "\n",
    "data_module=Tiny_BigEarthNetDataModule( \"./data/Tiny_BigEarthNet/regular\", \n",
    "                                       batch_size=16, \n",
    "                                       num_workers=4,\n",
    "                                       trans_modalities=modalities_trans,\n",
    "                                       trans_tokens=test_conf,\n",
    "                                       model=\"Atomiser\")\n",
    "data_module.setup()\n",
    "# Prepare dataloaders\n",
    "\n",
    "wand = False\n",
    "wandb_logger = None\n",
    "if wand:\n",
    "    if os.environ.get(\"LOCAL_RANK\", \"0\") == \"0\":\n",
    "        import wandb\n",
    "        wandb.init(\n",
    "            name=config_model['encoder'],\n",
    "            project=config_name_dataset+\"_modalities\",\n",
    "            config=config_model\n",
    "        )\n",
    "        wandb_logger = WandbLogger(project=config_name_dataset+\"_modalities\")\n",
    "\n",
    "#def __init__(self, config, wand, name)\n",
    "model = Model(config_model,wand=wand, name=xp_name,transform=test_conf)\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_ap\", min_delta=0.00, patience=15, verbose=False, mode=\"max\")\n",
    "\n",
    "profiler = AdvancedProfiler(dirpath=\"profiling\", filename=\"profiler_output.txt\")\n",
    "\n",
    "# Configure the trainer for distributed training.\n",
    "trainer = Trainer(\n",
    "    use_distributed_sampler=False,  # we use our custom sampler\n",
    "    #strategy=\"ddp\",\n",
    "    max_epochs=config_model[\"trainer\"][\"epochs\"],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=[early_stop_callback],\n",
    "    profiler=profiler  \n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec496c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
